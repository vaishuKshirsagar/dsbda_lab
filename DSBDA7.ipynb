{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f9e712-4f19-4240-ab82-37035cc09712",
   "metadata": {},
   "source": [
    "Assignment 7\n",
    "TCOD73 - Vaishnavi Kshiirsagar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca52a1d3-a7ab-474d-8c90-7f8e9aab5091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vaishnavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vaishnavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vaishnavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vaishnavi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "541fa4b4-160d-48fe-839c-82d328befa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document\n",
    "document = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, \n",
    "computer science, and artificial intelligence concerned with the \n",
    "interactions between computers and human language, in particular how \n",
    "to program computers to process and analyze large amounts of natural \n",
    "language data. Challenges in natural language processing frequently \n",
    "involve speech recognition, natural language understanding, and \n",
    "natural language generation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e064b38a-5e4e-41b5-99ac-2583b465a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da4be183-96c5-46d8-8833-5f2c677dc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40924e69-7cd9-48fa-a9cd-b417bd21f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a299b300-3a9b-4e64-891d-6b1a9e3ea008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74c656f4-0f61-4734-9592-62174b82ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e551cb47-f6da-46ad-bb65-1d70b2251d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency (TF) calculation\n",
    "tf = FreqDist(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b180c414-1729-49ba-af17-175dbb827d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Document Frequency (IDF) calculation\n",
    "corpus = [document]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5173b32a-c315-4f05-af58-0005160f881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preprocessed Document:\n",
      "Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', ',', 'natural', 'language', 'understanding', ',', 'and', 'natural', 'language', 'generation', '.']\n",
      "POS Tags: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), (',', ','), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('language', 'NN'), (',', ','), ('in', 'IN'), ('particular', 'JJ'), ('how', 'WRB'), ('to', 'TO'), ('program', 'NN'), ('computers', 'NNS'), ('to', 'TO'), ('process', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('large', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.'), ('Challenges', 'NNS'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('frequently', 'RB'), ('involve', 'VBP'), ('speech', 'NN'), ('recognition', 'NN'), (',', ','), ('natural', 'JJ'), ('language', 'NN'), ('understanding', 'NN'), (',', ','), ('and', 'CC'), ('natural', 'JJ'), ('language', 'NN'), ('generation', 'NN'), ('.', '.')]\n",
      "Filtered Tokens (Stop words removal): ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.', 'Challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', ',', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', '.']\n",
      "Stemmed Tokens: ['natur', 'languag', 'process', '(', 'nlp', ')', 'subfield', 'linguist', ',', 'comput', 'scienc', ',', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', ',', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data', '.', 'challeng', 'natur', 'languag', 'process', 'frequent', 'involv', 'speech', 'recognit', ',', 'natur', 'languag', 'understand', ',', 'natur', 'languag', 'gener', '.']\n",
      "Lemmatized Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', ',', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', '.', 'Challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', ',', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', '.']\n",
      "\n",
      "2. Term Frequency (TF):\n",
      "<FreqDist with 32 samples and 48 outcomes>\n",
      "\n",
      "3. Inverse Document Frequency (IDF):\n",
      "{'amounts': 1.0, 'analyze': 1.0, 'and': 1.0, 'artificial': 1.0, 'between': 1.0, 'challenges': 1.0, 'computer': 1.0, 'computers': 1.0, 'concerned': 1.0, 'data': 1.0, 'frequently': 1.0, 'generation': 1.0, 'how': 1.0, 'human': 1.0, 'in': 1.0, 'intelligence': 1.0, 'interactions': 1.0, 'involve': 1.0, 'is': 1.0, 'language': 1.0, 'large': 1.0, 'linguistics': 1.0, 'natural': 1.0, 'nlp': 1.0, 'of': 1.0, 'particular': 1.0, 'process': 1.0, 'processing': 1.0, 'program': 1.0, 'recognition': 1.0, 'science': 1.0, 'speech': 1.0, 'subfield': 1.0, 'the': 1.0, 'to': 1.0, 'understanding': 1.0, 'with': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"1. Preprocessed Document:\")\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "print(\"Filtered Tokens (Stop words removal):\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "print(\"\\n2. Term Frequency (TF):\")\n",
    "print(tf)\n",
    "print(\"\\n3. Inverse Document Frequency (IDF):\")\n",
    "print(dict(zip(vectorizer.get_feature_names_out(), idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075770d4-1df8-40d1-ab67-f90705fe101a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
